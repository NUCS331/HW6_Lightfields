{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on autoreload: https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy\n",
    "import scipy.misc\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.synthetic_aperture as synthetic_aperture\n",
    "import src.code as code\n",
    "import src.util_synthetic as util_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Part 2: Approximating a light-field with a Smartphone Camera</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of the homework, you will capture a light field by waving your tablet camera around. The goal of is to synthesize an image that appears to be captured with an aperture that is much larger aperture than your tablet camera [1]. \n",
    "\n",
    "The **advantage** of this technique is that captured images will appear to have a significantly smaller depth of field, giving the look and feel of a much more expensive camera. The other advantage is that, because we are capturing a light field, we can digitally refocus the image to any location we desire.\n",
    "\n",
    "The **disadvantage** of this technique is that, because it relies on a sequential image capture, it will only work for static scenes. The technique requires the image sequence to be registered, and the quality of the final photograph will be limited by how well the images can be registered. In this homework, we will use a simple correlation (or template-matching) based registration scheme.\n",
    "\n",
    "Reference:\n",
    "\n",
    "[1] Synthetic aperture confocal imaging, Marc Levoy, Billy Chen, Vaibhav Vaish, Mark Horowitz, Ian McDowall, and Mark Bolas. 2004. ACM Trans. Graph. 23, 3 (August 2004), 825-834. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](pictures/camera-motion.png)\n",
    "\n",
    "**Figure 1:** Capture an unstructured light field of a scene by waving a camera in front of a scene. Make sure the motion is in a plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](pictures/dataset_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** Four images from a video captured using the zig-zag motion shown in Figure 1. The red box shows the template that is used to register the video frames in the next part 2 of this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing and debugging your algorithm we are providing you with a sample movie dataset, so that you can be sure that the data is good enough for the algorithm to work. Your own data might be not ideal (in case you do something wrong) and it might not be working because of the data you have acquired.\n",
    "\n",
    "First you will load the video data into a large numpy array. Then you'll have to register each frame of captured video using a correlation function and generate synthetic aperture photos.\n",
    "\n",
    "Registration can be performed on either grayscale video frames or color images. You will use a simple template-matching scheme as described in the first lecture on image processing, but fortunately you don't have to implement this because there are many packages that do this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are more datailed instructions on what you need to do:\n",
    "\n",
    "1.    Use the **DATALOADER:**: We wrote a program to load the video into an n-darray from an .mp4 file\n",
    "2.    You can use all of the frames in the video, but this may make your processing slow if you captured more than a few seconds of video. You should be able to get away with selecting about 50 to 60 frames for processing (for instance by selecting only every few frames).\n",
    "3.    The red box in Figure 2 shows the template that was used for registration. You will need to select a similar template from the first frame of your video. You will then search for a match to this template in successive frames. The location of the match will tell you how many pixels your camera has shifted. Choose a size for this template (i.e. 16x16 pixels). You may need to adjust this to improve your results.\n",
    "4.    You only need to search for a template match within a window of your target frame. The window should be centered on the location of the template in the first frame. The size of the window should be slightly larger than the sum of the template size and the maximum shift of the target object over the entire set of video frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Task 1: Load the the data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the implementation for this dataset you want to replace this with your own movie file\n",
    "# For now you can keep it though\n",
    "path = 'data_part2/example_synthetic_aperture.MOV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're implementing this for you for the .mov file datatype. \n",
    "# It might work for you movie files out-of-the-box. But must likely you'll\n",
    "# need to adapt this slightly\n",
    "\n",
    "imgs = util_synthetic.read_video_to_array(path)\n",
    "\n",
    "imgs = imgs[:,:,:,::10] # only take every 10th frame\n",
    "\n",
    "print(imgs.shape) # Should be (Num_x, Num_y, 3, Num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(imgs[:,:,:,0]) # You might want to play around with this index to see that different images can be loaded correctly\n",
    "plt.title(\"An example image from the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Task 2: Crop the windows and template</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now you wille have to implement a function that crops the search window and the template out of the video data\n",
    "# You will return the cropped window for each frame and the template from to initial frame (the first one)\n",
    "#\n",
    "\n",
    "window_size = 150 # The size of the window. You can vary this if wanted.\n",
    "template_size = 30 # The template size. This parameter might need to be varied to work well\n",
    "\n",
    "focus_center = (120,210)\n",
    "\n",
    "window, template, top_left_search_window, top_left_template= synthetic_aperture.crop_search_window(imgs,window_size,template_size,focus_center)\n",
    "\n",
    "\n",
    "print(window.shape) # Should be (window_size,window_size,3,num_mgs)\n",
    "print(template.shape) # Should be (template_size, template_size, 3)\n",
    "\n",
    "print(\"Top-Left Corner of Search Window\")\n",
    "print(top_left_search_window) # 1D array with 2 entries for x and y\n",
    "print(\"Top-Left Corner of Template Window\")\n",
    "print(top_left_template) # 1D array with 2 entries for x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Subtask: Visualize what you've done !!! </span>\n",
    "\n",
    "We have implemnted a visualization function for you to sanity check if your cropping axctually works.\n",
    "\n",
    "Feel free to make any changes to this plotting function as you like. If you want to visualize something different please do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function also has the optional argument of plotting the used template\n",
    "plt.figure(figsize=(11,4))\n",
    "util_synthetic.plot_image_with_crop(imgs[:,:,:,0],window[:,:,:,0],top_left_search_window)\n",
    "\n",
    "# This function also has the optional argument of plotting the used template\n",
    "plt.figure(figsize=(11,4))\n",
    "util_synthetic.plot_image_with_crop(window[:,:,:,0],template[:,:,:])\n",
    "\n",
    "# This function also has the optional argument of plotting the used template at the same time if you pass at is an argument\n",
    "plt.figure(figsize=(11,4))\n",
    "util_synthetic.plot_image_with_crop(imgs[:,:,:,0],window[:,:,:,0],top_left_search_window,template,top_left_template)\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"image_window_and_template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Subtask: Visualize the captured lightfield</span>\n",
    "\n",
    "Visualize the search-window throughout the complete video data. You'll see how the camera has moved. This should remind you a lot to part1 of this homework. However, now we don't know the exact position where we've moved the camera too. This will be subject of this homework to figure out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "util_synthetic.visualizePixelShift(window)\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"visualize_pixelshift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Task 3: Template matching</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](pictures/search_window.png)\n",
    "\n",
    "Example how the windows search field and the template should look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](pictures/pixelshift_pattern.png)\n",
    "\n",
    "**Figure 4:** The pixel shifts for the video in Figure 1 found from Equation 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">MATH: Some math to understand what is going on</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will have to perform an autocorrelation of the template with the extracted search window. If your template $t[n,m]$ is $T \\times T$ pixels and your window $w[n,m]$ is $W \\times W $ pixels, you will compute the normalized correlation function $\\Delta[i,j]$ using the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$$\n",
    "\\Lambda[i, j]=\\frac{1}{N \\cdot P} \\sum_{n=1}^{T} \\sum_{i=1}^{T} t[n, m] \\cdot w_{i j}[n, m]\n",
    "$$<\\h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $w_{ij}$ is a block of pixels taken from the window $w$ that is centered on the pixel location $(i,j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "$$\n",
    "w_{i j}[n, m]=w^{2}\\left[n+i-\\frac{W}{2}, m+j-\\frac{W}{2}\\right]\n",
    "$$ \n",
    "<\\h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the normalization factors are computed as\n",
    "\n",
    "<h1>\n",
    "$$\n",
    "N=\\sum_{n=1}^{T} \\sum_{m=1}^{T} t^{2}[n, m]\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "P=\\sum_{n=1}^{T} \\sum_{m=1}^{T} w_{i j}^{2}[n, m]\n",
    "$$\n",
    "<\\h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the normalized factor P is computed over a window the size of the template ( pixels) not over the entire window ( pixels)!\n",
    "\n",
    "EXTRA POINTS: If you want you can implement this correlation function by yourself. It is very instructive and actually very helpful for understanding this homework. If you want to do this, you can the numpy function conv to do the sliding window operation of Equation the equation above.\n",
    "\n",
    "However: In order to save time you can simply use many of the pre-implemented correlation functions available in several Python packages, such as openCV. We will guide you through this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have calculated the correlation functions, you will find the pixel shift $[s_x,s_y]$ for each frame of video through following computation as:\n",
    "\n",
    "<h1>\n",
    "$$\n",
    "\\left[s_{x}, s_{y}\\right]=\\underset{i, j}{\\operatorname{argmax}} \\Delta [i, j](4)\n",
    "$$\n",
    "<\\h1>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Subtask (a): Template matching</span>\n",
    "\n",
    "We recommend using openCV's template matching function to do time-efficient matching.\n",
    "\n",
    "You can read more on this here:\n",
    "\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the methods of finding correlation between the template and a part of the frame \n",
    "\n",
    "methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR',\n",
    "            'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']\n",
    "\n",
    "# We will be using \"cv2.TM_CCOEFF_NORMED\" since this works reasonable well. But you're free to use different versions of this!\n",
    "method = methods[1] \n",
    "print(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's correlate the template with the original. You will have to implement this function\n",
    "res,top_left_correlation = synthetic_aperture.findCorrelation(window[:,:,:,0],template,method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the output. We've already implemented this for you, so this should work just by executing this cell\n",
    "\n",
    "print(top_left_correlation)\n",
    "print(res.shape) \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(res)\n",
    "code.add_colorbar(im)\n",
    "\n",
    "plt.subplot(122)\n",
    "im = plt.imshow(res)\n",
    "code.add_colorbar(im)\n",
    "plt.scatter(top_left_correlation[0],top_left_correlation[1],color='r')\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"image_correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Subtask (b): Template matching of a different image</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's correlate the template with a differenrt frame !\n",
    "\n",
    "img_idx = 20\n",
    "res,top_left_correlation = synthetic_aperture.findCorrelation(window[:,:,:,img_idx],template)\n",
    "print(top_left_correlation)\n",
    "\n",
    "\n",
    "# When we visualize this, you'll hopefully realize that the correlation maximum-spot has now slightly moved to a different positions!\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(res)\n",
    "code.add_colorbar(im)\n",
    "\n",
    "plt.subplot(122)\n",
    "im = plt.imshow(res)\n",
    "code.add_colorbar(im)\n",
    "plt.scatter(top_left_correlation[1],top_left_correlation[0],color='r')\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"image_correlation_different_frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">TASK: Correlate all images</span>\n",
    "\n",
    "Now you have to implement a function that is correlating all images in your video. Simply loop through all frames and apply the findCorrelation function that you've implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated, top_left_correlated_list = synthetic_aperture.findCorrelationAll(window,template,method)\n",
    "\n",
    "print(correlated.shape) # Should be (Num_imgs,W-w+1,W-w+1) with W window size and w template size\n",
    "print(type(top_left_correlated_list)) # Should be a ndarrau\n",
    "print(top_left_correlated_list.shape) # should be 2 x Num_imgs\n",
    "print(top_left_correlated_list.dtype) # should be integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">TASK: Visualize the correlation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the maximum spot (the red one) overlays with the visible maximum in the correlation images\n",
    "\n",
    "# Plot with maximum point denoted\n",
    "plt.figure(figsize = (14,5.5))\n",
    "util_synthetic.visualize_correlation_output(correlated,top_left_correlated_list,True)\n",
    "\n",
    "# plot without the maximal point shown to verify that there is actually an minimum\n",
    "plt.figure(figsize = (14,5.5))\n",
    "util_synthetic.visualize_correlation_output(correlated,top_left_correlated_list,False)\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"correlation_all_frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">TASK: Correct pixel locations of correlation output</span>\n",
    "\n",
    "Now we will have to add the template size to pixel_shifts that you have just calculated to get the coordinates in the original image coordinates!\n",
    "\n",
    "WHY: Have a look at the output image of the correlated images. This is specific to the correlation function as implemented in openCV.\n",
    "\n",
    "See documentation here: https://docs.opencv.org/master/d4/dc6/tutorial_py_template_matching.html\n",
    "\n",
    "HINT: If input image is of size (WxH) and template image is of size (wxh), output image will have a size of (W-w+1, H-h+1). Once you got the result, you can use cv.minMaxLoc() function to find where is the maximum/minimum value. Take it as the top-left corner of rectangle and take (w,h) as width and height of the rectangle. That rectangle is your region of template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to implement this function\n",
    "top_left_correlated = synthetic_aperture.correct_correlation_result_to_image_coordinates(top_left_correlated_list,imgs,window,template)\n",
    "\n",
    "print(top_left_correlated.dtype) # Should be int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\">Subtask: Visualize the new pixel locations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function \"display_new_image_location\" and \"display_new_image_location_all\" which we've already implemented for you to see if everything works as it is supposed to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_synthetic.display_new_image_location(imgs,top_left_correlated,top_left_search_window,window_size,5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the following cell (display_new_image_location_all( will show you an overview of the frames and will also display a red rectangle indicated where the match of the correlation function was found.\n",
    "\n",
    "It should always show the same part of the image, however at different locations. If this is not the case, go back and try to debug where your problem might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "util_synthetic.display_new_image_location_all(imgs,top_left_correlated,top_left_search_window,window_size)\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"display_new_image_location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\">Subtask: Analze the quality of registration</span>\n",
    "\n",
    "When you execute the following cell, you will the registered cropped image of the search window that you have chosen. These should look all the same! But watch out carefully. Those only are registered, however they are actually not the same because they are captured from different viwe positions.\n",
    "\n",
    "The red mark denotes the center of the image. This should be the same feature in all those images since that's where the maximum peak in the correlation algorihtm has been identified. If the center of those images is not the feature that you have chosen, something went wrong you you should go back to your implementation and try to debug.\n",
    "\n",
    "**Question for you:** Can you spot where other features in the search-window are not the same ?! This indicates that this is actually an approximation of a lightfield, similair to Part 1 of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "util_synthetic.display_new_image_location_all(imgs,top_left_correlated,top_left_search_window,window_size,display_full=False)\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"display_new_location_cropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Subtask: Visualize Pixel Shift Positions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of visualizing pictures, we're now visualizing the shift positions in a plot. We should then see the trajectory of the smartphone. This should be an almost zig-zag curve which aprooxximately covers a 2D grid, similair to the lightfield capture from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_shifts = synthetic_aperture.calculate_pixel_shifts(top_left_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pixel_shifts[1],pixel_shifts[0])\n",
    "plt.plot(pixel_shifts[1],pixel_shifts[0])\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid()\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"pixel_shifts_zig_zag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Problem 2: Refocus the images</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem will now focus on refocusing your captured lightfield to any position in the image.\n",
    "\n",
    "Once you have calculated the pixel shift, you can generate a synthetic aperture photograph simply by shifting the images of each frame of video in the opposite direction and then summing up the result. **Think about why it has to be shifted in negative way**\n",
    "\n",
    "If the $f_i$ is the ith video frame, and $s_x^i$ and $ s_y^i$ is the shift for that frame, you can simply calculate your synthetic aperture photograph as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "$$\n",
    "P[n, m]=\\sum_{i=1}^{N} f_{i}\\left[n-s_{x}^{i}, m-s_{y}^{i}\\right]\n",
    "$$\n",
    "<\\h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you:** Can you see the resemblance to the refocusing equation in Part 1 of the homework? This is actually quite similair!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 2 example images that focus on the eye of the spider and on the northwestern bottle. Try to reproduce those results!\n",
    "\n",
    "![test](pictures/Synthetic_2.png)\n",
    "\n",
    "Figure (a): Synthethic aperture photo focused on eye\n",
    "\n",
    "![test](pictures/Synthetic_3.png)\n",
    "\n",
    "Figure (b): Synthethic aperture photo focused on Northwestern bottle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Task 1: Translate one image</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will translate the images with the amount specified by the pixel_shift. For this you will have to implement a simple translate_image method. You can use any package of your liking to this. I recommend using openCV for speed. \n",
    "\n",
    "HINT: You can use parts of your implementation from Part 1 for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on you you've implemented the pixel_shift operations\n",
    "# You might need to switch dx and dy. Here's the combination that worked for me\n",
    "# Think about why I multiply with -1 !!! (you might need this too)\n",
    "\n",
    "k = 10\n",
    "\n",
    "dx = -1*pixel_shifts[1,k]\n",
    "dy = -1*pixel_shifts[0,k]\n",
    "img_translated = synthetic_aperture.translate_image(imgs[:,:,:,k],dx,dy)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img_translated)\n",
    "plt.title(\"Example of shifted image\")\n",
    "\n",
    "x0 = top_left_search_window[0]\n",
    "x1 = x0 + window_size\n",
    "y0 = top_left_search_window[1]\n",
    "y1 = y0 + window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Sanity check: Has the image been registered correctly?</span>\n",
    "Use this code to check if you registration approach has actually worked.\n",
    "\n",
    "The following code should show on the left the original image, in the middle the original image that you want to register before shiftin and on the right the image after shifting.\n",
    "\n",
    "The left and right image should now more or less overlap. If they are not registered, revisit your correlation or translation functions and try to identify the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(131)\n",
    "plt.imshow(imgs[x0:x1,y0:y1,:,0])\n",
    "plt.title(\"Initial frame window\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(imgs[x0:x1,y0:y1,:,k])\n",
    "plt.title(\"#\" +str(k) + \" window unregistered\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(img_translated[x0:x1,y0:y1,:])\n",
    "plt.title(\"#\" +str(k) + \" window registered\")\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"proof_image_shift_and_registration_works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Task: Translate all images</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've successully implemented a function that can translate one image, you'll now have to implement a function that translates all images according to the calculated pixel_shifts.\n",
    "\n",
    "You can use a for-loop for looping through the images and call the translate_image that you've implemented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_translated = synthetic_aperture.translate_all_images(imgs,pixel_shifts)\n",
    "\n",
    "print(imgs.shape) # Those should have the same size now\n",
    "print(imgs_translated.shape) # Those should now have the same size as imgs\n",
    "print(imgs_translated.dtype) # Can be float or uint8. Doesn't really mattert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualize how well the registration has worked. You can use the functions that we've already implemented for you to do this. This is simply to check that everything you've implemented is actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">**Sanity check:** Did registration work for the complete dataset?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_plots = 4\n",
    "n_plots = 4\n",
    "\n",
    "x0 = top_left_search_window[0]\n",
    "x1 = x0 + window_size\n",
    "y0 = top_left_search_window[1]\n",
    "y1 = y0 + window_size\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "util_synthetic.visualizePixelShift(imgs_translated[x0:x1,y0:y1,:,:])\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"image_registration_after_shifting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "util_synthetic.visualizePixelShift(imgs_translated,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to check that the feature that you registered against is actually in the center now !\n",
    "plt.figure(figsize=(15,15))\n",
    "util_synthetic.show_registered_images(imgs_translated,top_left_search_window,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Task: Average all images to get refocused</span>\n",
    "\n",
    "After translation and registering the images all that is left to do is to average over all images. Then the final result should be the refocused image.\n",
    "\n",
    "If this image looks weird or not as expected, try to identify where the problem is and solve it. You can write on campuswire and describe what you did, what worked and where you're think you got stock and we'll try to help you figuering it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_synthethic = synthetic_aperture.average_images(imgs_translated)\n",
    "print(img_synthethic.shape)\n",
    "print(img_synthethic.dtype)\n",
    "\n",
    "plt.imshow(img_synthethic)\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"image_synthethic_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Problem: Build the synthethic aperture pipeline</span>\n",
    "\n",
    "Now we have implemented the refocusing algorithm for only specific point. However, we'd like to have an algorithm that can focus on any point in the image. \n",
    "\n",
    "This is easy. All we have to do is choose a different center of focus. We then redefine the sesarch-window and the template of the first frame. Then we do exactly the same pipeline we have developped above and we're done.\n",
    "\n",
    "In order to save some time for you, we have already implemented this function for you. You can check it out under \"calculate_synthethic_aperture_image\". Just by glancing over, you should be immediately see what steps we are performing and you should recognize everything that you've implemented in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by choosing a different point of focus. Here we're choosing the northwestern bottle as indicated as the blue point on the image.\n",
    "\n",
    "focus_center=(165,105)\n",
    "plt.imshow(imgs[:,:,:,0])\n",
    "plt.scatter(focus_center[1],focus_center[0],color='b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that is left is to run the refocusing pipeline. Hopefully it works out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_synthethic = util_synthetic.calculate_synthethic_aperture_image(imgs,focus_center)\n",
    "\n",
    "print(img_synthethic.shape)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img_synthethic)\n",
    "plt.title(\"Image focused at \" + str(focus_center))\n",
    "\n",
    "util_synthetic.save_fig_as_png(\"image_synthethic_northwestern_bottle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\">Task: Play around with interactive refocusing widget</span>\n",
    "\n",
    "Use the following cell which has some code for you that does interactive refocusing and play around with the algorithm. You can use this interactive viewer later for you own synthethic aperture video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y,refocus):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[:,:,:,0])\n",
    "    plt.scatter(y,x,color='r')\n",
    "    \n",
    "    focus_center = (x,y)\n",
    "    \n",
    "    print(focus_center)\n",
    "    plt.title(\"Image focused at \" + str(focus_center))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    if refocus:\n",
    "        plt.figure(figsize=(15,10))\n",
    "        img_synthethic = util_synthetic.calculate_synthethic_aperture_image(imgs,focus_center,window_size,template_size)\n",
    "        plt.imshow(img_synthethic)\n",
    "        plt.title(\"Image focused at \" + str(focus_center))\n",
    "\n",
    "print(window_size)\n",
    "print(template_size)\n",
    "\n",
    "x_widget =  widgets.IntSlider(value = window_size/2,min=window_size/2+1, max=imgs.shape[0]-1-window_size/2, step=1)\n",
    "y_widget =  widgets.IntSlider(value = window_size/2,min=window_size/2+1, max=imgs.shape[1]-1-window_size/2, step=1)\n",
    "    \n",
    "interactive_plot = interactive(f, x = x_widget, y = y_widget, refocus = False)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '15'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Task: Implement the refocusing algorithm</span>\n",
    " \n",
    "Now you will refocus on a new object. To do this you will simply select your template from the first from of video to be centered on a different object. Then all you need to do are repeat steps 3 and 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Problem: Use your own video</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Task 1: Capture an unstructured light field with your camera</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You don't have to do this now. You can do this AFTER you've implement your algorithm since we are providing you with an example movie which is ensured to work fine.\n",
    "\n",
    "You can use any Android/iPhone camera application that records a video. You don't even have to use a smartphone. Any camerea device will do the job. You will capture a video as you wave your camera around in front of a static scene.\n",
    "\n",
    "Here are some guidelines that should help you capture a video:\n",
    "\n",
    "\n",
    "1.    You need to avoid tilting and rotating the camera as much as possible. You should just shift the camera in a plane (see Figure 1). You may need to try this several times before you capture a good video.\n",
    "2.    You should press the phone against a flat object (book, wall, etc.) to help guide the planar motion\n",
    "3.    Experiment with different types of camera motion. You can try moving the camera in a zig-zag motion (as in Figure 1). You can also try using circular motion. In general, the more you cover the plane, the better your results in the next section will be.\n",
    "4.    Make sure your video is not too long. You should capture a few seconds of video.\n",
    "5.    Capture a scene with a few objects at different depths. See Figure 2 for an example. In the next section you will create narrow synthetic aperture images that are focused on these objects\n",
    "6.    Make sure that all of the objects in the scene are in focus. As long as objects are about a meter in size and 1-2 meters away, they will more or less be in focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](pictures/camera-motion.png)\n",
    "\n",
    "**Figure 1:** Capture an unstructured light field of a scene by waving a camera in front of a scene. Make sure the motion is in a plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Task 2: Load and synthesize your own large aperture image</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = util_synthetic.read_video_to_array(path)\n",
    "imgs = imgs[:,:,:,::10] # only take every 10th frame\n",
    "print(imgs.shape) # Should be (Num_x, Num_y, 3, Num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
